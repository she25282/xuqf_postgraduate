### Researches on Knowledge Distillation in Recommender System

- Graph Structure Aware Contrastive Knowledge Distillation for Incremental Learning in Recommender Systems 【CIKM 2021】【[paper](https://dl.acm.org/doi/pdf/10.1145/3459637.3482117)】
- Topology Distillation for Recommender System 【KDD 2021】【[paper](https://arxiv.org/pdf/2106.08700)】
- Target Interest Distillation for Multi-Interest Recommendation 【CIKM 2022】【[paper](https://dl.acm.org/doi/pdf/10.1145/3511808.3557464)】【[code](https://github.com/THUwangcy/ReChorus/tree/CIKM22)】
- On-Device Next-Item Recommendation with Self-Supervised Knowledge Distillation 【SIGIR 2022】【[paper](https://arxiv.org/pdf/2204.11091.pdf)】【[code](https://github.com/xiaxin1998/OD-Rec)】
- Cross-Task Knowledge Distillation in Multi-Task Recommendation 【AAAI 2022】【[paper](https://ojs.aaai.org/index.php/AAAI/article/view/20352/20111)】
- Gradient Matching for Categorical Data Distillation in CTR Prediction 【Recsys 2023】【[paper](https://dl.acm.org/doi/pdf/10.1145/3604915.3608769)】【[code](https://rixwew.github.io/pytorch-fm/)】
- Ensemble Modeling with Contrastive Knowledge Distillation for Sequential Recommendation 【SIGIR 2023】【[paper](https://arxiv.org/pdf/2304.14668.pdf)】【[code](https://github.com/hw-du/EMKD)】
- Data-free Knowledge Distillation for Reusing Recommendation Models 【Recsys 2023】【[paper](https://dl.acm.org/doi/pdf/10.1145/3604915.3608789)】
